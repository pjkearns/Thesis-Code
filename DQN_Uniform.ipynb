{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4mr0grvgEIIH"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy import matlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from FH_Functions import opt_fracs, get_truncated_normal\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "ujF51HlOETVp"
   },
   "outputs": [],
   "source": [
    "#@title Qnet\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=16, fc2_units=16):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "b-cy1kfhqONW",
    "outputId": "c467b449-4830-41bb-f40b-f4240286c92c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 1.00            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 1e-3               # learning rate \n",
    "UPDATE_EVERY = 10       # how often to update the network\n",
    "\n",
    "\"\"\"GPU only faster for larger networks\"\"\"\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "-HXpQNEzEUoh"
   },
   "outputs": [],
   "source": [
    "#@title Agent, Replay\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        # Compute Q targets for current states \n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "bkrt1RftEV70"
   },
   "outputs": [],
   "source": [
    "#@title UNIFORM Environment\n",
    "class DQNUniformLineSearchEnv():\n",
    "    def __init__(self, lam = 0.4, Nsamples = 5, Nstates = 501):\n",
    "         \n",
    "        self.lam      = lam\n",
    "        self.Nactions = 101\n",
    "        self.Nsamples = Nsamples\n",
    "        self.S        = np.linspace(0,1,Nstates)\n",
    "        self.reset()\n",
    "\n",
    "    def step(self, action, theta):  # return the next state \n",
    "        done = False\n",
    "        thtMin = self.thtMin\n",
    "        thtMax = self.thtMax\n",
    "        Xc = self.Xc\n",
    "        Xo = self.Xo \n",
    "        N  = self.state[1]\n",
    "        \n",
    "        size   = thtMax - thtMin        \n",
    "        act    = 0.5 * (action / (self.Nactions-1))\n",
    "        dist   = size * act\n",
    "        dist   = self.S[np.argmin(np.abs(self.S - dist))]\n",
    "\n",
    "        if Xc < theta:\n",
    "            Xc = Xc + dist\n",
    "        else:\n",
    "            Xc = Xc - dist\n",
    "            \n",
    "        Xc = self.S[np.argmin(np.abs(self.S - Xc))]\n",
    "        \n",
    "        if Xc < theta:\n",
    "            thtMin = Xc\n",
    "            Xo = thtMax\n",
    "        else:\n",
    "            thtMax = Xc\n",
    "            Xo = thtMin\n",
    "            \n",
    "        newSize = thtMax - thtMin\n",
    "            \n",
    "        reward = size - newSize - self.lam*dist\n",
    "        \n",
    "        self.thtMin = thtMin\n",
    "        self.thtMax = thtMax\n",
    "        self.Xc     = Xc\n",
    "        self.Xo     = Xo\n",
    "        self.state  = np.array([self.lam, N-1, self.thtMax - self.thtMin])\n",
    "        \n",
    "        if self.state[1] == 0:\n",
    "            done = True\n",
    "        return self.state, reward, done\n",
    "    \n",
    "    def reset(self):       \n",
    "        self.thtMin = 0\n",
    "        self.thtMax = 1\n",
    "        self.Xc     = 0\n",
    "        self.Xo     = 1\n",
    "        self.state  = np.array([self.lam, self.Nsamples, self.thtMax - self.thtMin])\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "R6AFL0QYEYFb"
   },
   "outputs": [],
   "source": [
    "#@title DQN\n",
    "def dqn(agent, env, n_episodes=2000, eps_start=1.0, eps_end=0.1, eps_decay=0.9999):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "        target_scores (float): average scores aming to achieve, the agent will stop training once it reaches this scores\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "\n",
    "    np.random.seed(1)\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        # Reset env and score at the beginning of episode\n",
    "        thtRange = np.random.rand(100)\n",
    "        \n",
    "        for tht in thtRange:\n",
    "            state = env.reset()               # get the current state\n",
    "            score = 0                         # initialize the score\n",
    "            done  = False\n",
    "\n",
    "            while not done:\n",
    "                action = agent.act(state, eps)            \n",
    "                next_state, reward, done = env.step(action, tht)            \n",
    "                agent.step(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                score += reward\n",
    "                if done:\n",
    "                    break \n",
    "            scores_window.append(score)       # save most recent score\n",
    "            scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tEps: {:.4f}'.format(i_episode, np.mean(scores_window), eps), end=\"\")\n",
    "        \n",
    "        if i_episode % 1000 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}\\tEps: {:.4f}'.format(i_episode, np.mean(scores_window), eps))\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N8PkMPMOEZeD"
   },
   "outputs": [],
   "source": [
    "Nsteps = 5\n",
    "env   = DQNUniformLineSearchEnv(Nsamples = Nsteps)\n",
    "agent = Agent(state_size=3, action_size=101, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "4ruzOu6lLqoJ",
    "outputId": "2368519a-c717-45cf-80e1-354eaaacbed8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000\tAverage Score: 0.65\tEps: 0.6065\n",
      "Episode 2000\tAverage Score: 0.66\tEps: 0.3678\n",
      "Episode 3000\tAverage Score: 0.63\tEps: 0.2230\n",
      "Episode 4000\tAverage Score: 0.68\tEps: 0.1353\n",
      "Episode 5000\tAverage Score: 0.69\tEps: 0.1000\n",
      "Episode 5050\tAverage Score: 0.65\tEps: 0.1000"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "scores = dqn(agent, env, n_episodes=5050, eps_decay = 0.9995)\n",
    "stop = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "4ruzOu6lLqoJ",
    "outputId": "2368519a-c717-45cf-80e1-354eaaacbed8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1705.87\n"
     ]
    }
   ],
   "source": [
    "print(\"time: %.2f\"%(stop - start))\n",
    "# torch.save(agent.qnetwork_local.state_dict(), './Policies/DQN_Uniform_ReLU.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Load in pre-saved agent if not re-training\"\"\"\n",
    "# agent = Agent(state_size=3, action_size=101, seed=0)\n",
    "# agent.qnetwork_local.to(device)\n",
    "# agent.qnetwork_local.load_state_dict(torch.load('./Policies/DQN_Uniform_ReLU.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "5LAqx6p0E1j9",
    "outputId": "78387ec2-72fc-4d51-8e27-05fa58d3228f"
   },
   "outputs": [],
   "source": [
    "lam = 0.4\n",
    "states = np.arange(1,Nsteps + 1)\n",
    "DQNacts = np.zeros(Nsteps)\n",
    "cost = 0\n",
    "\n",
    "np.random.seed(0)\n",
    "thtRange = np.random.rand(1000)\n",
    "\n",
    "S = np.linspace(0,1,501)\n",
    "\n",
    "for theta in thtRange:\n",
    "    Xc = 0\n",
    "    Xo = 1\n",
    "    lb = 0\n",
    "    ub = 1\n",
    "    totDist = 0\n",
    "    for ii in states[::-1]:\n",
    "        length = ub - lb\n",
    "        stt = np.array([lam, ii, length])\n",
    "        action = agent.act(stt, 0)\n",
    "        action = 0.5*action/101\n",
    "        DQNacts[-ii] += action\n",
    "\n",
    "        dist = length*action\n",
    "        dist = S[np.argmin(np.abs(S - dist))]\n",
    "        totDist += dist\n",
    "        \n",
    "        if Xc < theta:   \n",
    "            Xc += dist\n",
    "\n",
    "        else:\n",
    "            Xc -= dist\n",
    "            \n",
    "        Xc = S[np.argmin(np.abs(S - Xc))]\n",
    "\n",
    "        if Xc < theta:\n",
    "            lb = Xc\n",
    "            Xo = ub\n",
    "        else:\n",
    "            ub = Xc\n",
    "            Xo = lb\n",
    "\n",
    "    cost += lam*totDist + (ub - lb)\n",
    "    \n",
    "DQNacts /= 1000\n",
    "cost /= 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.286081609837131"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*(.3578-cost)/.3578"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "5LAqx6p0E1j9",
    "outputId": "78387ec2-72fc-4d51-8e27-05fa58d3228f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions:  [0.13366337 0.32575743 0.2904604  0.26894059 0.25482178]\n",
      "Average cost:  0.33888640000000275\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(states,DQNacts)\n",
    "plt.ylim([0, 0.5])\n",
    "print(\"Actions: \", DQNacts)\n",
    "print(\"Average cost: \", cost)  \n",
    "np.save('./DQN_bestAction_Uniform_lam4_N5_s501_A101',DQNacts)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DQN_uniform.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
